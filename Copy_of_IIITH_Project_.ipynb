{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prashaiml/Flickr8k-Project1/blob/main/Copy_of_IIITH_Project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from numpy import array\n",
        "import glob\n",
        "import cv2\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Nre881Cv7htN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NN6QY9db3rtt"
      },
      "outputs": [],
      "source": [
        "# creating directory to save Kaggle Json file and giving permissions for the file\n",
        "import os\n",
        "os.makedirs('/root/.kaggle',exist_ok= True)\n",
        "\n",
        "# Move the kaggle.json file to the correct location\n",
        "os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "\n",
        "# Set permissions for the kaggle.json file\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "k_evmSi84c-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download ZipFile form Kaggle Datasets\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = '/content/flickr8k.zip'\n",
        "\n",
        "unzip_file_path = '/content/flickr8k'\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_file_path)\n",
        "\n",
        "print(f\"Files extracted to: {unzip_file_path}\")\n",
        "os.listdir('/content/flickr8k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuUDNfFr6V1D",
        "outputId": "43bb9d55-8532-4b78-b4d2-0351af48ce76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
            "License(s): CC0-1.0\n",
            "Downloading flickr8k.zip to /content\n",
            " 99% 1.03G/1.04G [00:06<00:00, 141MB/s]\n",
            "100% 1.04G/1.04G [00:06<00:00, 175MB/s]\n",
            "Files extracted to: /content/flickr8k\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['captions.txt', 'Images']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = glob.glob('/content/flickr8k/Images/*.jpg')\n",
        "image_filenames = [os.path.basename(path) for path in images]\n",
        "print(len(image_filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-I0K6828VyJ",
        "outputId": "67d83289-51eb-482f-c91c-82f72785d513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean And STD calculation of given dataset\n",
        "def calculate_mean_std(image_paths):\n",
        "    # Initialize variables to calculate mean and std\n",
        "    total_pixels = 0\n",
        "    channel_sum = np.zeros(3)  # For R, G, B\n",
        "    channel_sum_squared = np.zeros(3)  # For R^2, G^2, B^2\n",
        "\n",
        "    for image_path in tqdm(image_paths, desc=\"Processing images\"):\n",
        "        # Load the image in RGB format\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (224, 224))  # Resize to a common size\n",
        "\n",
        "        # Normalize pixel values to [0, 1]\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Update sums for mean and variance calculation\n",
        "        total_pixels += image.shape[0] * image.shape[1]\n",
        "        channel_sum += np.sum(image, axis=(0, 1))\n",
        "        channel_sum_squared += np.sum(image ** 2, axis=(0, 1))\n",
        "\n",
        "    # Calculate mean and std\n",
        "    mean = channel_sum / total_pixels\n",
        "    variance = (channel_sum_squared / total_pixels) - (mean ** 2)\n",
        "    std = np.sqrt(variance)\n",
        "\n",
        "    return mean, std\n",
        "M,S = calculate_mean_std(images)"
      ],
      "metadata": {
        "id": "IAXtykIuZSxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(M,S)"
      ],
      "metadata": {
        "id": "9Y6TZBqhaDnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = [0.45754746, 0.44561315, 0.40343669]\n",
        "S = [0.27459249, 0.26735015, 0.2818728 ]"
      ],
      "metadata": {
        "id": "VJyW82ipxtUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a to display images\n",
        "def display_images(pictures,preprocess_imgs):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  if pictures :\n",
        "    for i,image_path in enumerate(pictures[:10]):\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.subplot(2,5,i+1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')  # Hide axis\n",
        "  elif preprocess_imgs is not None:\n",
        "    for i, image_data in enumerate(preprocess_imgs[:10]):\n",
        "            image_data = np.transpose(image_data, (1, 2, 0))  # Transpose preprocessed image data\n",
        "            image = (image_data * 255).astype(np.uint8)  # Assuming image_data is normalized\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            plt.subplot(2, 5, i + 1)\n",
        "            plt.imshow(image)\n",
        "            plt.axis('off')  # Hide axis\n",
        "\n",
        "  plt.show()\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "xaixshiv8hbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_images(images,None)"
      ],
      "metadata": {
        "id": "f2uD-u3OqzAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#image preprocessing\n",
        "def preprocess_image_opencv(image_path,mean,std):\n",
        "\n",
        "    # Step 1: Load the image in RGB format\n",
        "    image = cv2.imread(image_path)  # Load the image (BGR format)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n",
        "\n",
        "    # Step 2: Resize the image to (224, 224)\n",
        "    image = cv2.resize(image, (224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # Step 3: Normalize pixel values to [0, 1]\n",
        "    image = image.astype(np.float32) / 255.0\n",
        "\n",
        "    # Step 4: Normalize pixel values using mean and std\n",
        "    image = (image - mean) / std\n",
        "\n",
        "    # Step 5: Transpose to (3, 224, 224) format (channels first)\n",
        "    image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "zu9U8hT-frXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_images = []  # List to store preprocessed images\n",
        "\n",
        "for image_path in images:\n",
        "    preprocessed_image = preprocess_image_opencv(image_path,M,S)\n",
        "    preprocessed_images.append(preprocessed_image)\n",
        "\n",
        "\n",
        "print(f\"Number of preprocessed images: {len(preprocessed_images)}\")\n",
        "print(f\"Shape of first preprocessed image: {preprocessed_images[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOkJn-E6gDLS",
        "outputId": "5ec000dd-74e1-4595-88f8-129d5fd20057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of preprocessed images: 8091\n",
            "Shape of first preprocessed image: (3, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_captions(caps):\n",
        "    print(len(caps))\n",
        "\n",
        "    # Step 1: Tokenize and clean captions\n",
        "    def clean_caption(caption):\n",
        "      caption = caption.lower()  # Convert to lowercase\n",
        "      caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)  # Remove punctuation\n",
        "      caption = \" \".join(caption.split())  # Remove extra spaces\n",
        "      words = caption.split()  # Tokenize caption into words\n",
        "      unique_words = sorted(set(words))  # Collect unique words and sort alphabetically\n",
        "      return unique_words\n",
        "\n",
        "    tokenized_captions = [clean_caption(caption) for caption in captions]\n",
        "\n",
        "\n",
        "    # Step 2: Build vocabulary\n",
        "    word_counts = Counter(word for caption in tokenized_captions for word in caption)\n",
        "    vocab = [word for word, count in word_counts.items()]\n",
        "\n",
        "\n",
        "    # Step 3: Add special tokens\n",
        "    special_tokens = [\"<bos>\", \"<eos>\", \"<unk>\",\"<pad>\",]\n",
        "    vocab = special_tokens + vocab\n",
        "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    # Step 4: Encode captions with special tokens\n",
        "    def encode_caption(caption):\n",
        "        encoded = [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in caption]\n",
        "        encoded = [word_to_index[\"<bos>\"]] + encoded + [word_to_index[\"<eos>\"]]\n",
        "        return encoded\n",
        "\n",
        "    encoded_captions = [encode_caption(caption) for caption in tokenized_captions]\n",
        "\n",
        "    def pad_captions(captions, max_length):\n",
        "      encoded_captions = []\n",
        "      for caption in captions:\n",
        "          if len(caption) < max_length:\n",
        "              padded_cap = caption + [word_to_index.get('<pad>')] * (max_length - len(caption))\n",
        "              encoded_captions.append(padded_cap)\n",
        "          else:\n",
        "              encoded_captions.append(caption[:max_length])\n",
        "      return encoded_captions\n",
        "\n",
        "    equal_encoded_captions = pad_captions(encoded_captions, 16)\n",
        "\n",
        "    return word_to_index, equal_encoded_captions\n"
      ],
      "metadata": {
        "id": "jF1ffVcZblS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_df = pd.read_csv('/content/flickr8k/captions.txt')\n",
        "captions = captions_df['caption'].tolist()\n",
        "\n",
        "word_index, equal_encoded_captions = preprocess_captions(captions)\n",
        "\n",
        "encoded_16 = 0\n",
        "others = 0\n",
        "for cap in equal_encoded_captions:\n",
        "  if len(cap) == 16:\n",
        "    encoded_16+=1\n",
        "  else:\n",
        "    others+=1\n",
        "print(\"encoded to Sizeof 16:\",encoded_16,\"\\t others:\",others)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGgfGnPa_d_0",
        "outputId": "d432b68f-c8af-49ca-d399-55968496bb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40455\n",
            "encoded to Sizeof 16: 40455 \t others: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8Hb3EByzHwI",
        "outputId": "7d52ada4-6caa-4d16-f8fe-ab01f4dd3bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8832"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "captions_df['captions']=equal_encoded_captions"
      ],
      "metadata": {
        "id": "i1cjYOYJdf9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_df = pd.DataFrame({'image': image_filenames, 'Preprocessed_images': preprocessed_images})"
      ],
      "metadata": {
        "id": "AN6u2AcjbRow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.merge(images_df, captions_df, on='image', how='inner')"
      ],
      "metadata": {
        "id": "pGPJaBULeYi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.drop(\"image\",axis=1,inplace=True)\n",
        "final_df.drop(\"caption\",axis=1,inplace=True)\n"
      ],
      "metadata": {
        "id": "U7jUkxvEiUYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split the DataFrame into train and validation sets\n",
        "train_df, val_df = train_test_split(final_df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "4BbJOI-BlviV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Get the row corresponding to the index\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        # Extract preprocessed image and caption\n",
        "        image = row[\"Preprocessed_images\"]  # List of arrays\n",
        "        caption = row[\"captions\"]  # List of integers (encoded captions)\n",
        "\n",
        "        # Convert them to torch tensors\n",
        "        image_tensor = torch.tensor(image, dtype=torch.float32)  # Ensure it's float32 for model input\n",
        "        caption_tensor = torch.tensor(caption, dtype=torch.long)  # Ensure it's long for tokenized captions\n",
        "\n",
        "        return image_tensor, caption_tensor\n"
      ],
      "metadata": {
        "id": "gFOLxG8AekKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects\n",
        "train_dataset = ImageCaptionDataset(train_df)\n",
        "val_dataset = ImageCaptionDataset(val_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "scfbESdBhHVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_idx, (images, captions) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}:\")\n",
        "    print(f\"Image Tensor Shape: {images.shape}\")\n",
        "    print(f\"Caption Tensor Shape: {captions.shape}\")\n",
        "    print(f\"No.of batches in train_df:{len(train_loader)}\\n\")\n",
        "\n",
        "    print(\"Shape of single image and caption pair:\\n\")\n",
        "    print(f\"Single Image Shape: {images[0].shape}\")\n",
        "    print(f\"Single Caption Shape: {captions[0].shape}\")\n",
        "    print(images[0])\n",
        "    print(captions[0])\n",
        "\n",
        "    # Break after inspecting the first batch (optional)\n",
        "    break"
      ],
      "metadata": {
        "id": "kVUwjQKQhRKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNImageEmbedding(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(CNNImageEmbedding, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply conv1, ReLU activation, and max pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten to Batch size x (128 * 28 * 28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # to get the final embedding\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pZvMBHs9sFab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMCaptionEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMCaptionEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.clamp(x, 0, self.embedding.num_embeddings - 1)\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, (hn, cn) = self.lstm(x)\n",
        "        caption_embedding = hn[-1]  # Use the hidden state from the last time step\n",
        "        caption_embedding = self.fc(caption_embedding)\n",
        "        return caption_embedding\n"
      ],
      "metadata": {
        "id": "pOS078IzwfFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, cnn, lstm, output_dim):\n",
        "        super(ImageCaptionModel, self).__init__()\n",
        "        self.cnn = cnn\n",
        "        self.lstm = lstm\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        # Get image embeddings from the CNN\n",
        "        image_embeddings = self.cnn(images)\n",
        "\n",
        "        # Get caption embeddings from the LSTM\n",
        "        caption_embeddings = self.lstm(captions)\n",
        "\n",
        "        # Calculate the MSE between the image and caption embeddings\n",
        "        loss = nn.MSELoss()(image_embeddings, caption_embeddings)\n",
        "        return loss, image_embeddings, caption_embeddings\n"
      ],
      "metadata": {
        "id": "sUg-zLerwtA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def calculate_cosine_similarity(image_embeddings, caption_embeddings):\n",
        "    # Cosine similarity requires reshaping the embeddings to (batch_size, -1)\n",
        "    # If your embeddings are 2D, flatten them to 1D\n",
        "    image_embeddings = image_embeddings.view(image_embeddings.size(0), -1)\n",
        "    caption_embeddings = caption_embeddings.view(caption_embeddings.size(0), -1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity = F.cosine_similarity(image_embeddings, caption_embeddings)\n",
        "    return similarity\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(image_embeddings, caption_embeddings, threshold=0.7):\n",
        "    # Calculate cosine similarity between image and caption embeddings\n",
        "    similarity = calculate_cosine_similarity(image_embeddings, caption_embeddings)\n",
        "\n",
        "    # Determine if the similarity is above the threshold\n",
        "    correct = (similarity > threshold).float()\n",
        "\n",
        "    # Accuracy is the fraction of correct predictions\n",
        "    accuracy = correct.sum() / correct.size(0)\n",
        "    return accuracy\n",
        "\n",
        "# Training loop with accuracy calculation\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Hyperparameters\n",
        "output_dim = 512  # Desired embedding size for both image and caption\n",
        "vocab_size = len(word_index)  # Size of the vocabulary\n",
        "embedding_dim = 256  # Embedding size for the words\n",
        "hidden_dim = 512  # LSTM hidden size\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Check if GPU is available, otherwise default to CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the CNN, LSTM, and combined model\n",
        "cnn_model = CNNImageEmbedding(output_dim).to(device)\n",
        "lstm_model = LSTMCaptionEmbedding(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "model = ImageCaptionModel(cnn_model, lstm_model, output_dim).to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# GradScaler for mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    for images, captions in train_loader:\n",
        "        # Move the batch to the GPU\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Mixed precision training with autocast\n",
        "        with autocast():\n",
        "            # Forward pass: get image and caption embeddings\n",
        "            loss, image_embeddings, caption_embeddings = model(images, captions)\n",
        "\n",
        "            # Calculate the MSE loss between the image and caption embeddings\n",
        "            loss = F.mse_loss(image_embeddings, caption_embeddings)\n",
        "\n",
        "        # Backward pass: compute gradients and update weights using scaler\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient clipping to avoid exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Step the optimizer (scaled gradients)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = calculate_accuracy(image_embeddings, caption_embeddings)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_accuracy = total_accuracy / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Accuracy: {avg_accuracy * 100}%\")\n"
      ],
      "metadata": {
        "id": "0LSwmX2iw21z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07956d92-42ba-418e-a33a-fce1999e44ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.0005181284117459209, Accuracy: 97.45553359683794%\n",
            "Epoch 2/3, Loss: 1.9005738313074623e-06, Accuracy: 99.97838438735178%\n",
            "Epoch 3/3, Loss: 3.0448548067514303e-07, Accuracy: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def calculate_cosine_similarity(image_embeddings, caption_embeddings):\n",
        "    image_embeddings = image_embeddings.view(image_embeddings.size(0), -1)\n",
        "    caption_embeddings = caption_embeddings.view(caption_embeddings.size(0), -1)\n",
        "    similarity = F.cosine_similarity(image_embeddings, caption_embeddings)\n",
        "    return similarity\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(image_embeddings, caption_embeddings, threshold=0.7):\n",
        "    similarity = calculate_cosine_similarity(image_embeddings, caption_embeddings)\n",
        "    correct = (similarity > threshold).float()  # 1 if similarity > threshold, else 0\n",
        "    accuracy = correct.sum() / correct.size(0)  # Mean accuracy for the batch\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on the validation set\n",
        "def evaluate(model, val_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for images, captions in val_loader:\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            # Forward pass: get image and caption embeddings\n",
        "            loss, image_embeddings, caption_embeddings = model(images, captions)\n",
        "\n",
        "            # Calculate the MSE loss between the image and caption embeddings\n",
        "            loss = F.mse_loss(image_embeddings, caption_embeddings)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            accuracy = calculate_accuracy(image_embeddings, caption_embeddings)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += accuracy.item()\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    avg_accuracy = total_accuracy / len(val_loader)\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "# Validation phase\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "val_loss, val_accuracy = evaluate(model, val_loader, device)\n",
        "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy * 100}%\")\n"
      ],
      "metadata": {
        "id": "QVjtErq9w5ol",
        "outputId": "ff52b1e9-846e-4cf1-ec3a-876820489f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.936390227785888e-07, Validation Accuracy: 100.0%\n"
          ]
        }
      ]
    }
  ]
}